}
if (!require(glmnet)) {
install.packages("glmnet")
}
if (!require(ISLR2)) {
install.packages("ISLR2")
}
if (!require(leaps)) {
install.packages("leaps")
}
#Load libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(faraway)
library(car)
library(gridExtra)
library(glmnet)
library(ISLR2)
library(leaps)
census_api_key("2547c95ce33b1ed0eec3aafd0fd8526a5bb9a22e")
#Pull data set for specific variables
puma_data <- get_pums(variables = c("AGEP","SCHL","PINCP", "SEX", "RAC1P", "TEN", "OCCP", "MAR", "ACR")
,state = "OR"
,year = 2022)
#Rename the columns
puma_data <- puma_data %>% dplyr::rename(Age = AGEP,
EducationLevel = SCHL,
Income = PINCP,
Sex = SEX,
PersonNumber = SPORDER,
Race = RAC1P,
Homeownership = TEN,
Occupation = OCCP,
MaritalStatus = MAR,
HouseAcreage = ACR
)
head(puma_data[, 2:8])
#Remove columns that are not needed
puma_data <- puma_data %>%
select(-ST, -SERIALNO, -WGTP, -PWGTP)
head(puma_data)
#Convert education levels
puma_data <- puma_data %>%
mutate(
EducationLevel = case_when(
EducationLevel == "bb" ~ "00",
TRUE ~ EducationLevel
),
EducationLevel = as.numeric(EducationLevel)
)
#Make sure it is numeric
class(puma_data$EducationLevel)
#Convert Occupation to numerical
puma_data <- puma_data %>%
mutate(
Occupation = case_when(
Occupation == "bbbb" ~ "0000",
Occupation == "000N" ~ "0000",
TRUE ~ Occupation
),
Occupation = as.numeric(Occupation)
)
#Make sure it is numeric
class(puma_data$Occupation)
#Convert homeownership
puma_data <- puma_data %>%
mutate(
Homeownership = case_when(
Homeownership == "b" ~ "0",
TRUE ~ Homeownership
),
Homeownership = as.numeric(Homeownership)
)
#Make sure it is numeric
class(puma_data$Homeownership)
#Convert HouseAcreage
puma_data <- puma_data %>%
mutate(
HouseAcreage = case_when(
HouseAcreage == "b" ~ "0",
TRUE ~ HouseAcreage
),
HouseAcreage = as.numeric(HouseAcreage)
)
#Make sure it is numeric
class(puma_data$HouseAcreage)
##Group education level
puma_data <- puma_data %>%
mutate(
EducationGroup = cut(EducationLevel, breaks = c(0, 16, 21, 22, Inf),
labels = c("Less than HS", "HS, GED, or Associates Degree",
"Bachelors Degree", "Masters Degree or higher"),
right = FALSE)
)
unique(puma_data$EducationGroup)
puma_data[1:50,]
#Drop age below 14
puma_data <- puma_data %>%
filter(Age >= 14)
head(puma_data[puma_data$Age < 14,])
#Drop income that are less than 1 from the data set
#Drop observation with 0 and negative income
puma_data <- puma_data %>% filter(Income >= 1)
head(puma_data)
sum(puma_data$Income<1)
#relabeling race
puma_data$Race <- as.numeric(puma_data$Race)
puma_data <- puma_data %>%
mutate(
Race = case_when(
Race == 1 ~ "White",
Race == 2 ~ "Black/AfricanAmerican",
between(as.numeric(Race), 3, 6) ~ "NativeAmerican/Alaskan",
between(as.numeric(Race), 6, 7) ~ "Asian/PacificIslander",
between(as.numeric(Race), 8, 9) ~ "Other/MoreThanOne",
TRUE ~ as.character(Race)
)
)
puma_data$Race[1:100]
#relabeling MaritalStatus
puma_data <- puma_data %>%
mutate(
MaritalStatus = case_when(
MaritalStatus == "1" ~ "Married",
MaritalStatus == "2" ~ "Widowed",
MaritalStatus == "3" ~ "Divorced",
MaritalStatus == "4" ~ "Separated",
MaritalStatus == "5" ~ "Not Married/Under 15",
TRUE ~ as.character(MaritalStatus)
)
)
#Checking labels are assigned.
unique(puma_data$MaritalStatus)
puma_data$MaritalStatus[11200:11300]
#relabeling Acreage
puma_data <- puma_data %>%
mutate(
HouseAcreage = case_when(
HouseAcreage == 0 ~ "Not a one-family home",
HouseAcreage == 1 ~ "< 1 Acre",
HouseAcreage == 2 ~ "1 - 10 Acres",
HouseAcreage == 3 ~ "> 10 Acres",
TRUE ~ as.character(HouseAcreage)
)
)
#Checking labels are assigned.
unique(puma_data$HouseAcreage)
puma_data$HouseAcreage[1500:1600]
#relabeling tenure
puma_data <- puma_data %>%
mutate(
Homeownership = case_when(
Homeownership == 0 ~ "N/A",
Homeownership == 1 ~ "Owned with mortgage or loan",
Homeownership == 2 ~ "Owned free and clear",
Homeownership == 3 ~ "Rented",
Homeownership == 4 ~ "Occupied without payment of rent",
TRUE ~ as.character(Homeownership)
)
)
#Checking labels are assigned.
unique(puma_data$Homeownership)
puma_data$Homeownership[11200:11300]
#Identify gender
puma_data <- puma_data %>%
mutate(
Sex = case_when(
Sex == 1 ~ "Male",
Sex == 2 ~ "Female",
TRUE ~ as.character(Sex)
)
)
#Checking labels are assigned.
unique(puma_data$Sex)
head(puma_data$Sex)
#Occupation Grouping
puma_data <- puma_data %>%
filter(as.numeric(Occupation) >= 0010 & as.numeric(Occupation) < 9920) %>%
mutate(
OccupationGroup = case_when(
between(as.numeric(Occupation), 0010, 3550) ~ "Professional/Technical",
between(as.numeric(Occupation), 3600, 4160) ~ "Healthcare/FoodServices",
between(as.numeric(Occupation), 4200, 7640) ~ "WhiteCollar/BlueCollar",
between(as.numeric(Occupation), 7700, 9760) ~ "Manufacturing/Transportation",
between(as.numeric(Occupation), 9800, 9830) ~ "Military",
TRUE ~ "Not Classified"
)
)
#Checking labels are assigned.
unique(puma_data$OccupationGroup)
puma_data$OccupationGroup[2020:2120]
puma_data$Occupation[puma_data$OccupationGroup == "Not Classified"]
puma_data_raw <- puma_data
head(puma_data_raw)
#Rearrange the columns
puma_data <- puma_data %>%
select(EducationGroup, EducationLevel, OccupationGroup, Occupation, Income, Age, Sex, Race, Homeownership, PersonNumber)
head(puma_data)
#Summarizing the ranges of numerical
summary(puma_data$Age) #Range of Age
summary(puma_data$Income) #Range of Age
summary(puma_data$EducationLevel) #Range of Education
table(puma_data$EducationGroup) #Count of each level of education
table(puma_data$Race) #Count of each race
table(puma_data$Homeownership) #Count of each Homeownership
table(puma_data$OccupationGroup) #Count of each Occupationgroup
#Plot income distribution
ggplot(puma_data %>% filter(Income > 0), aes(Income)) +
geom_histogram(binwidth = 10000) +
scale_x_continuous(labels = function(x) paste0(x / 1000, "K")) +
labs(title = "Income Distribution in Oregon"
, x = "Income by $10k"
, y = "Count of Individuals") +
theme_minimal()
# Does higher education mean higher income accounting for sex and age?
model_1 <- lm(Income ~ EducationGroup + Sex + Age, data = puma_data)
model_2 <- lm(Income ~ EducationGroup + Sex * Age, data = puma_data)
#Compare model 1 and 2
anova(model_1, model_2)
#The full model provides a significantly better fit.
#Linearity and Homoscedasticity assumption
plot(fitted(model_2), residuals(model_2))
#Normality assumption
qqnorm(residuals(model_2))
qqline(residuals(model_2))
#Independence assumptions
plot(residuals(model_2))
#Fitted vs residuals plot exhibiting pattern
#Logged models
model_3 <- lm(log(Income) ~ EducationGroup + Sex + Age, data = puma_data, na.action = na.exclude)
model_4 <- lm(log(Income) ~ EducationGroup + Sex * Age, data = puma_data, na.action = na.exclude)
#Compare models 3 and 4
anova(model_3, model_4)
#Model 4 is the best from 2 models
#Linear regression assumption
plot(fitted(model_4), residuals(model_4))
#Normality assumption
qqnorm(residuals(model_4))
qqline(residuals(model_4))
#Independence assumptions
plot(residuals(model_4))
#Add observation ID
puma_data$ID <- 1:nrow(puma_data)
## Attaching the Case Influence Statistics with the Data
Data <- fortify(model_4, puma_data)
## Plot the Case Influence Statistics for each observation (subject)
par(mfrow=c(1,3))
qplot(ID,.hat, data = Data)
qplot(ID,.stdresid, data = Data)
qplot(ID,.cooksd, data = Data)
#There is evidence of outliers
#Test for influential observations
#Cut off point is 4/(n-k-2)
cutoff <- 4/((nrow(puma_data)-length(model_4$coefficients)-2))
#Calculate Cook's distance
cook.d <- cooks.distance(model_4)
#Plot to identify observation with Cook's distance higher than cutoff
plot(cook.d, pch=".", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = cutoff, col="red")  # add cutoff line
text(x=1:length(cook.d)+1, y=cook.d, labels=ifelse(cook.d>cutoff, names(cook.d),""), col="red")  # add labels
# Removing Outliers
# influential row numbers
influential <- as.numeric(names(cook.d)[(cook.d > cutoff)])
#Create another data frame without influential observations
puma_data_2 <- puma_data[-influential, ]
#Fit a regression without influential
model_5 <- lm(log(Income) ~ EducationGroup + Sex * Age, data = puma_data_2, na.action = na.exclude)
#Compare the summary before and after removing outliers
summary(model_4)
summary(model_5)
par(mfrow=c(2,2))
#Linear regression assumption
plot(fitted(model_5), residuals(model_5))
#Normality assumption
qqnorm(residuals(model_5))
qqline(residuals(model_5))
#Independence assumptions
plot(residuals(model_5))
#Multi-colinearlity
vif(model_5)
#Fit ridge regression
x <- model.matrix(log(Income) ~ EducationGroup + Sex * Age, puma_data_2)[, -1]
y <- log(puma_data_2$Income)
ridge_model <- glmnet(x, y, alpha = 0)
summary(ridge_model)
#perform 10 fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
#produce plot of test MSE by lambda value
plot(cv_model)
#The lambda value that minimizes the test MSE turns out to be 0.04258514
#find coefficients of model with best lambda
model_6 <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(model_6)
#produce Ridge trace plot
plot(ridge_model, xvar = "lambda")
puma_data_q2 <- puma_data |>
select(EducationGroup, Income, Race, OccupationGroup)
head(puma_data_q2)
summary(puma_data_q2)
#Data visualization
puma_data_q2_race_income <- puma_data_q2 |>
group_by(Race, Income) |>
summarise(n = n())
ggplot(puma_data_q2_race_income, aes(x = Income)) +
geom_histogram(binwidth = 5000, fill = "skyblue", color = "black") +
facet_wrap(~Race) +
labs(title = "Distribution of Income by Race", x = "Income", y = "Frequency")
#Data visualization
puma_data_q2_occ_income <- puma_data_q2 |>
group_by(OccupationGroup, Income) |>
summarise(n = n())
ggplot(puma_data_q2_occ_income, aes(x = Income)) +
geom_histogram(binwidth = 5000, fill = "skyblue", color = "black") +
facet_wrap(~OccupationGroup) +
labs(title = "Distribution of Income by Occupation Group and Race", x = "Income", y = "Frequency")
#Validation set and CV
#Separate data into 2 groups, training and testing data
set.seed(538)
train <- sample(c(TRUE, FALSE), nrow(puma_data_q2), replace = TRUE)
test <- (!train)
#apply regsubsets() to the training set in order to perform best subset selection
regfit.best <- regsubsets(Income ~ ., data = puma_data_q2[train, ], nvmax = 11)
#make a model matrix from the test data
test.mat <- model.matrix(Income ~ ., data = puma_data_q2[test, ])
#run a loop, and for each size i
val.errors <- rep(NA, 11)
for (i in 1:11) {
coefi <- coef(regfit.best, id = i)
pred  <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((puma_data_q2$Income[test] - pred)^2)
}
#Show MSE
val.errors
which.min(val.errors)
coef(regfit.best, 9)
#perform best subset selection on the full data set, and select the best 9 variable model
regfit.best <- regsubsets(Income ~ ., data = puma_data_q2, nvmax =11)
coef(regfit.best, 9)
k <- 10
n <- nrow(puma_data_q2)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
#Create predict function
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
for (j in 1:k) {
best.fit <- regsubsets(Income ~ .,
data = puma_data_q2[folds != j, ],
nvmax = 11)
for (i in 1:11) {
pred <- predict(best.fit, puma_data_q2[folds == j, ], id = i)
cv.errors[j, i] <- mean((puma_data_q2$Income[folds == j] - pred)^2) }
}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
#Cross-validation selects a 11-variable model
#perform best subset selection on the full data set in order to obtain the 11-variable model
#11 variables includes all variables in the dataset
reg.best <- regsubsets(Income ~ ., data = puma_data_q2, nvmax = 11)
coef(reg.best, 11)
head(puma_data_raw)
puma_data_q3 <- puma_data_raw |>
select(Homeownership, Race, MaritalStatus, OccupationGroup, Occupation, Income)
head(puma_data_q3)
puma_data_filtered <- puma_data_q3 |>
filter(Occupation != 0)
ggplot(puma_data_filtered, aes(x = Occupation)) +
geom_bar(fill = "skyblue", color = "black") +
labs(title = "Distribution of Occupations", x = "Occupation", y = "Count") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(puma_data_q3, aes(x = OccupationGroup, y = Occupation)) +
geom_bar(stat = "identity", fill = "skyblue", color = "black") +
labs(title = "The relationship between OccupationGroup and Occupation Size", x = "OccupationGroup", y = "Occupation Size") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(puma_data_q3, aes(x = Homeownership, y = Occupation)) +
geom_bar(stat = "identity", fill = "skyblue", color = "black") +
labs(title = "The relationship between Homeownership and Occupation Size", x = "Homeownership", y = "Occupation Size") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(puma_data_q3, aes(x = Race, fill = factor(Occupation))) +
geom_bar() +
labs(title = "Occupation vs. Homeownership by Race", x = "Race", y = "Count of Occupations") +
scale_fill_manual(values = c("White" = "blue", "Black" = "red")) +  # Customize colors for each race
theme(axis.text.x = element_text(angle = 45, hjust = 1))
model_1_q3 <- lm(Occupation ~ as.factor(MaritalStatus) + as.factor(OccupationGroup)
+ as.factor(Homeownership) + as.factor(Race) + Income, data = puma_data_q3)
summary(model_1_q3)
model_2_q3 <- lm(Occupation ~ as.factor(MaritalStatus) + as.factor(OccupationGroup)
+ as.factor(Homeownership) + Income, data = puma_data_q3)
summary(model_2_q3)
#Compare models 1 and model 2
anova(model_1_q3, model_2_q3)
plot(fitted(model_1_q3), residuals(model_1_q3))
qqnorm(residuals(model_1_q3))
qqline(residuals(model_1_q3))
plot(residuals(model_1_q3))
#Fitted vs residuals plot exhibiting pattern
model_3_q3 <- lm(Occupation ~ as.factor(MaritalStatus) * as.factor(OccupationGroup)
+ as.factor(Homeownership) + as.factor(Race) + Income, data = puma_data_q3)
model_4_q3 <- lm(Occupation ~ as.factor(MaritalStatus) + as.factor(OccupationGroup)
* as.factor(Homeownership) + as.factor(Race) + Income, data = puma_data_q3)
summary(model_3_q3)
summary(model_4_q3)
anova(model_3_q3, model_4_q3)
plot(fitted(model_3_q3), residuals(model_3_q3))
qqnorm(residuals(model_3_q3))
qqline(residuals(model_3_q3))
plot(residuals(model_3_q3))
#Fitted vs residuals plot exhibiting pattern
#Calculate median income for 50 YO males
coef <- coef(model_6)
lw_HS_m  <- coef[1, ] + coef[5, ] + (50*coef[6, ]) + (50*1*coef[7, ]) #lower than HS male
GED_m    <- coef[1, ] + coef[2, ] + coef[5, ] + (50*coef[6, ]) + (50*1*coef[7, ]) #HS, GED, Associate degree male
bach_m   <- coef[1, ] + coef[3, ] + coef[5, ] + (50*coef[6, ]) + (50*1*coef[7, ]) #Bachelor's degree male
master_m <- coef[1, ] + coef[4, ] + coef[5, ] + (50*coef[6, ]) + (50*1*coef[7, ]) #master or higher male
exp(lw_HS_m)
exp(GED_m)
exp(bach_m)
exp(master_m)
#Calculate median income for 50 YO females
lw_HS_f  <- coef[1, ] + (50*coef[6, ]) #lower than HS female
GED_f    <- coef[1, ] + coef[2, ] + (50*coef[6, ]) #HS, GED, Associate degree female
bach_f   <- coef[1, ] + coef[3, ] + (50*coef[6, ]) #Bachelor's degree female
master_f <- coef[1, ] + coef[4, ] + (50*coef[6, ]) #master or higher female
exp(lw_HS_f)
exp(GED_f)
exp(bach_f)
exp(master_f)
summary(model_1_q3)$coefficients
summary(model_1_q3)$r.squared
plot(model_1_q3)
train_index <- sample(1:nrow(puma_data_q3), 0.8 * nrow(puma_data_q3))  # 80% for training
train_data <- puma_data_q3[train_index, ]
test_data <- puma_data_q3[-train_index, ]
num_folds <- 10
model_1_q3 <- lm(Occupation ~ as.factor(MaritalStatus) + as.factor(OccupationGroup)
+ as.factor(Homeownership) + as.factor(Race) + Income, data = puma_data_q3)
test_rmse <- numeric(num_folds)
test_mae <- numeric(num_folds)
for (i in 1:num_folds) {
train_index <- sample(1:nrow(puma_data_q3), 0.8 * nrow(puma_data_q3))  # 80% for training
train_data <- puma_data_q3[train_index, ]
test_data <- puma_data_q3[-train_index, ]
model_1 <- lm(Occupation ~ as.factor(MaritalStatus) + as.factor(OccupationGroup)
+ as.factor(Homeownership) + as.factor(Race) + Income, data = train_data)
predictions <- predict(model_1, newdata = test_data)
test_rmse[i] <- sqrt(mean((test_data$Occupation - predictions)^2))
test_mae[i] <- mean(abs(test_data$Occupation - predictions))
}
avg_test_rmse <- mean(test_rmse)
avg_test_mae <- mean(test_mae)
# Print average test set performance
cat("Average Test RMSE:", avg_test_rmse, "\n")
cat("Average Test MAE:", avg_test_mae, "\n")
test_rmse <- numeric(num_folds)
test_mae <- numeric(num_folds)
for (i in 1:num_folds) {
train_index <- sample(1:nrow(puma_data_q3), 0.8 * nrow(puma_data_q3))  # 80% for training
train_data <- puma_data_q3[train_index, ]
test_data <- puma_data_q3[-train_index, ]
model_3 <- lm(Occupation ~ as.factor(MaritalStatus) * as.factor(OccupationGroup)
+ as.factor(Homeownership) + as.factor(Race) + Income, data = puma_data_q3)
predictions <- predict(model_3, newdata = test_data)
test_rmse[i] <- sqrt(mean((test_data$Occupation - predictions)^2))
test_mae[i] <- mean(abs(test_data$Occupation - predictions))
}
avg_test_rmse <- mean(test_rmse)
avg_test_mae <- mean(test_mae)
# Print average test set performance
cat("Average Test RMSE:", avg_test_rmse, "\n")
cat("Average Test MAE:", avg_test_mae, "\n")
#Test for influential observations
#Cut off point is 4/(n-k-2)
cutoff <- 4/((nrow(puma_data)-length(model_4$coefficients)-2))
#Calculate Cook's distance
cook.d <- cooks.distance(model_4)
#Plot to identify observation with Cook's distance higher than cutoff
plot(cook.d, pch=".", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = cutoff, col="red")  # add cutoff line
text(x=1:length(cook.d)+1, y=cook.d, labels=ifelse(cook.d>cutoff, names(cook.d),""), col="red")  # add labels
suppressWarnings({
#Multi-colinearlity
vif(model_5)})
suppressWarnings({
#Multi-colinearlity
vif(model_5, type = predictor)})
suppressWarnings({
#Multi-colinearlity
vif(model_5, type = "predictor")})
suppressWarnings({
#Multi-colinearlity
vif(model_5)})
, type = "predictor"
suppressWarnings({
#Multi-colinearlity
vif(model_5, type = "predictor")})
suppressWarnings({
#Multi-colinearlity
vif(model_5)})
