---
title: "Data Wizards (Group 4) Project 1 R Notebook"
author:
- Di Chen
- Mai Castellano
- Tyler Kussee
- Spencer Hutchison
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
## For your notes, an if statement for checking if a package is installed :)
#if(!require(somepackage)){
#    install.packages("somepackage")
#    library(somepackage)
#}

if (!require(tidycensus)) {
  install.packages("tidycensus")
}
if (!require(tidyverse)) {
  install.packages("tidyverse")
}
if (!require(dplyr)) {
  install.packages("dplyr")
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Load libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
library(ggplot2)

census_api_key("2547c95ce33b1ed0eec3aafd0fd8526a5bb9a22e")
```

# Introduction

The American Community Survey is an ongoing survey that provides vital information
on a yearly basis about our nation and it's people. The information from the survey 
generates data that helps inform how trillions of dollars in federal funds are distributed.

With the data from the American Community Survey, the Data Wizards have come up 
with the following 3 questions they would like to answer:

* Does higher education mean higher income accounting for sex and age?
* Can we predict household income based on education, occupation, and race/ethnicity?
* Is there a relationship between race/ethnicity and home ownership?

In particular, we'll be focusing on the first question to get started.

## Obtain the Data

In the case of the ACS dataset, we'll pull it via an API. We've already declared our key earlier, so now we just need to pull our dataset with the variables and filtering we'd like. In this case we'll want to pull our Age (AGEP), Education Level (SCHL), sex (SEX), and Total Person's Income (PINCP) while filtering to the year 2022 and the State of OR.

```{r echo=FALSE, message=FALSE, results='hide'}
#Pull data set for specific variables
puma_data <- get_pums(variables = c("AGEP","SCHL","PINCP", "SEX")
                      ,state = "OR"
                      ,year = 2022)
```

## Scrub the data

Since we've already pulled the data, now we need to just clean up the data first. Let's begin by assigning the column names more descriptive names:

```{r, error=TRUE, collapse=TRUE, warning=FALSE, echo=FALSE, results='hide'}
#Rename the columns
puma_data <- puma_data %>% rename(Age = AGEP,
                                  EducationLevel = SCHL,
                                  Income = PINCP,
                                  Sex = SEX,
                                  PersonNumber = SPORDER)
head(puma_data[, 2:5])
```

We can remove the the ST, Serial Number, and housing weight columns for now, 
since we don't really need that information. I'll keep PersonNumber in for now,
but we can remove it if not needed.

```{r, error=TRUE, collapse=TRUE, warning=FALSE, echo=FALSE, results='hide'}
#Remove columns that are not needed
puma_data <- puma_data %>% 
  select(-ST, -SERIALNO, -WGTP, -PWGTP)
head(puma_data)
```

Now we'll clean up some of this data. in this case we'll convert "bb" in our Education
level to a 0, and then convert our Educational level to numeric values.

```{r, message=FALSE, echo=FALSE, results='hide'}
#Convert education levels
puma_data <- puma_data %>%
  mutate(
    EducationLevel = case_when(
      EducationLevel == "bb" ~ "00",
      TRUE ~ EducationLevel
    ),
    EducationLevel = as.numeric(EducationLevel)
  )

#Make sure it is numeric
class(puma_data$EducationLevel)
```

Now we want to create groupings of our data. In this case, we'll be grouping education levels
based on if they've had "No Education", "Lower Than High School", "High School", 
"HS Diploma or GED", "Associates Degree", "Bachelors Degree", "Masters Degree", or
a "Doctoral Degree".

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
##Group education level
puma_data <- puma_data %>%
  mutate(
     EducationGroup = cut(EducationLevel, breaks = c(0, 2, 11, 16, 20, 21, 22, 24, Inf),
                         labels = c("No schooling completed", "Less than High School",
                                    "High School", "high school diploma or GED",
                                    "Associates Degree", "Bachelors Degree",
                                    "Masters Degree", "Doctorate Degree"),
                         right = FALSE)
  )
unique(puma_data$EducationGroup)
puma_data[1:50,]
```


Next I'll rearrange the columns:

```{r, error=TRUE, collapse=TRUE, warning=FALSE, echo=FALSE, results='hide'}
#Rearange the columns
puma_data <- puma_data %>%
  select(EducationGroup, EducationLevel, Income, Age, Sex, PersonNumber)
head(puma_data)
```

Since we're interested in the income of individuals, let's remove all ages younger than 14. The legal working age of Oregon is 14 onwards.

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Drop age below 14
puma_data <- puma_data %>%
  filter(Age >= 14)
head(puma_data[puma_data$Age < 14,])
```


Now we should work on the variable codes to convert them to their actual descriptions to make the content easier to peruse. We'll start with Education Level and convert it from it's characters to the actual descriptions.

```{r, error=TRUE, collapse=TRUE, warning=FALSE, echo=FALSE, results='hide'}
#Since we're refactoring the Education levels to groups, commenting this original out.
puma_data <- puma_data %>%
  mutate(
    EducationLevel = case_when(
      EducationLevel == 0 ~ "(less than 3 years old)",
      EducationLevel == 1 ~ "No schooling completed",
      EducationLevel == 2 ~ "Nursery School / Preschool",
      EducationLevel == 3 ~ "Kindergarten",
      EducationLevel == 4 ~ "Grade 1",
      EducationLevel == 5 ~ "Grade 2",
      EducationLevel == 6 ~ "Grade 3",
      EducationLevel == 7 ~ "Grade 4",
      EducationLevel == 8 ~ "Grade 5",
      EducationLevel == 9 ~ "Grade 6",
      EducationLevel == 10 ~ "Grade 7",
      EducationLevel == 11 ~ "Grade 8",
      EducationLevel == 12 ~ "Grade 9",
      EducationLevel == 13 ~ "Grade 10",
      EducationLevel == 14 ~ "Grade 11",
      EducationLevel == 15 ~ "Grade 12 - no diploma",
      EducationLevel == 16 ~ "Regular high school diploma",
      EducationLevel == 17 ~ "GED or alternative credential",
      EducationLevel == 18 ~ "Some college, but less than 1 year",
      EducationLevel == 19 ~ "1 or more years of college credit, no degree",
      EducationLevel == 20 ~ "Associates Degree",
      EducationLevel == 21 ~ "Bachelors Degree",
      EducationLevel == 22 ~ "Masters Degree",
      EducationLevel == 23 ~ "Professional Degree beyond a Bachelors Degree",
      EducationLevel == 24 ~ "Doctorate Degree",
      TRUE ~ as.character(EducationLevel)
    )
)
puma_data[1:100,]
```

Now lets identify the genders in the dataset:

```{r, error=TRUE, collapse=TRUE, warning=FALSE, echo=FALSE, results='hide'}
#Identify gender
puma_data <- puma_data %>%
  mutate(
    Sex = case_when(
      Sex == 1 ~ "Male",
      Sex == 2 ~ "Female",
      TRUE ~ as.character(Sex)
    )
)
head(puma_data)
```

With the data mostly cleaned up, we can now explore the data.

## Explore the data

We want to get a sense for the data that we'll be working with so first we'll find the range of ages that we have in our dataset. This shows that our mean and median age is very close together (43.1 and 43 respectively) showing a good distribution of ages; with a max age of 95. Education level has a similar mean and median (16.31, and 18 respectively), and it gives us a good way to  picture our distribution as well.  Unlike our age distribution the income mean and median are a bit off (35,364 and 21,000 respectively). This is something we may need to look into more in a graphical view. We also provide a table of the levels of education and the their counts.

```{r, error=TRUE, collapse=TRUE, warning=FALSE}
#Summarizing the ranges of numerical 
summary(puma_data$Age) #Range of Age
summary(puma_data$Income) #Range of Age
summary(puma_data$EducationNumber) #Range of Education
table(puma_data$EducationLevel) #Count of each level of education
```

Next we'll take a look into the income distribution in a graphical view. For this view we will also remove those that are below 0. This allows us to see a very long tailed view of the income distribution for our dataset. Our mean being the tallest point in the graph being around that $21,000 we saw in our exploration.

```{r, error=TRUE, collapse=TRUE, warning=FALSE}
#Plot inceom distribution
ggplot(puma_data %>% filter(Income > 0), aes(Income)) +
  geom_histogram(binwidth = 10000) +
  scale_x_continuous(labels = function(x) paste0(x / 1000, "K")) +
  labs(title = "Income Distribution in Oregon"
       , x = "Income by $10k"
       , y = "Count of Individuals") +
  theme_minimal()
```

[Reference on Variables for now](https://usa.ipums.org/usa/resources/codebooks/DataDict1822.pdf)

## Model the data

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Drop zero income from the data set
#Drop observation with 0 income (unemployed individuals)
puma_data <- puma_data[puma_data$Income != 0, ]
puma_data
head(puma_data)
```

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
# Does higher education mean higher income accounting for sex and age?
model_1 <- lm(Income ~ EducationGroup + Sex + Age, data = puma_data)
model_2 <- lm(Income ~ EducationGroup + Sex * Age, data = puma_data)
model_3 <- lm(Income ~ EducationGroup * Sex + Age, data = puma_data)
model_4 <- lm(Income ~ EducationGroup * Sex * Age, data = puma_data)

#Compare model 3 and 4
anova(model_3, model_4)

#The full model provides a significantly better fit.

#Compare model 2 and 4
anova(model_2, model_4)

#Model 4 is a better fit.

#Compare model 1 and 4
anova(model_1, model_4)
```
From four regression models that are considered. There are convincing evidence that the model that include the interaction between education and sex, education and age, sex and age, and 3 ways interaction between education, sex, and age is a better fit. We will use this model to answer the question of interest.

##Check the assumption

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE, fig.show='hide'}
#Linearity and Homoscedasticity assumption
plot(fitted(model_4), residuals(model_4))

#Normality assumption
qqnorm(residuals(model_4))
qqline(residuals(model_4))

#Independence assumptions
plot(residuals(model_4))

#Fitted vs residuals plot exhibiting pattern
```

Consider log transformation of the response variable:

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Logged models
model_5 <- lm(log(Income) ~ EducationGroup + Sex + Age, data = puma_data, na.action = na.exclude)
model_6 <- lm(log(Income) ~ EducationGroup + Sex * Age, data = puma_data, na.action = na.exclude)
model_7 <- lm(log(Income) ~ EducationGroup * Sex + Age, data = puma_data, na.action = na.exclude)
model_8 <- lm(log(Income) ~ EducationGroup * Sex * Age, data = puma_data, na.action = na.exclude)

#Compare models 7 and 8
anova(model_7, model_8)

#Compare models 6 and 8
anova(model_6, model_8)

#Compare models 5 and 8
anova(model_5, model_8)

#Model 8 is the best from 4 models
```
```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Linear regression assumption
plot(fitted(model_8), residuals(model_8))

#Normality assumption
qqnorm(residuals(model_8))
qqline(residuals(model_8))
```
The first plot displays the fitted values versus the residuals from model 8. The observations are clustered around the y = 0 line, indicating that the residuals are distributed relatively evenly above and below zero. The clustering pattern changes around the fitted value of approximately 9.5, where more observations appear. However, there is no obvious systematic pattern in the residuals, suggesting that the assumptions of homoscedasticity and linearity are met.

The QQ plot exhibits some deviation from normality. However, linear regression is known to be relatively robust against deviations from normality, particularly when the sample size is large. Overall, model 8 appears to be a good fit for the data, as the assumptions of homoscedasticity, linearity, and normality of residuals are reasonably met.

## Interpret the data

To answer our question, does higher education mean higher income accounting for sex and age? Use the fitted model to compare the income across all 8 levels of education for a 50 years old males and females in Oregon.

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Use predict() to fit the regression and find the average income
new_data_1 <- data.frame(EducationGroup = c("No schooling completed","Less than High School", "High School", "high school diploma or GED","Associates Degree", "Bachelors Degree", "Masters Degree", "Doctorate Degree"),
                       Sex = "Male",
                       Age = 50)

mu_1 <- predict(model_8, newdata = new_data_1 , type = "response")
exp(mu_1)
```

It appears that the average income for a 50-year-old male in Oregon varies significantly across different educational categories. Starting with individuals who have no schooling completed, their average income is \$19,927.16, slightly lower than the average income for those with less than a high school education, which stands at \$21,855.60. Surprisingly, individuals with only a high school education (without a diploma) experience a decrease in average income to $14,441.65, suggesting that completing high school does not necessarily lead to improved income prospects for this demographic.

However, there is a notable increase in average income for individuals with a high school diploma or GED, reaching \$26,611.61. From this point onward, it seems that higher levels of education correspond to higher average incomes for this demographic. For instance, individuals with an associate's degree experience a substantial increase in average income to \$37,603.97. Further up the educational ladder, those with a bachelor's degree see a significant jump in average income to \$53,105.31. Pursuing even higher education, individuals with a master's degree experience a further increase in average income to \$74,404.36, while those with a doctorate degree have the highest average income of $87,990.00.

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}
#Use predict() to fit the regression and find the average income
new_data_2 <- data.frame(EducationGroup = c("No schooling completed","Less than High School", "High School", "high school diploma or GED","Associates Degree", "Bachelors Degree", "Masters Degree", "Doctorate Degree"),
                       Sex = "Female",
                       Age = 50)

mu_2 <- predict(model_8, newdata = new_data_2 , type = "response")
exp(mu_2)
```

The average income for 50-year-old females in Oregon also exhibits variations across different educational categories. Starting with individuals who have no schooling completed, their average income is \$13,670.589. This increases slightly for those with less than a high school education, reaching \$14,888.971. Surprisingly, individuals with only a high school education (without a diploma) experience a decrease in average income to $9,206.923, similar to the trend observed among males.

However, there is a significant increase in average income for females with a high school diploma or GED, rising to \$17,306.203. This trend continues as educational attainment increases. For instance, individuals with an associate's degree experience a substantial increase in average income to \$24,280.252. Further up the educational ladder, those with a bachelor's degree see a significant jump in average income to \$33,974.807.

Despite these initial similarities, males tend to experience higher average incomes compared to females in the higher educational categories. For example, males with a bachelor's, master's, or doctorate degree generally have higher average incomes than their female counterparts with the same level of education.

## Appendix

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
## An if statement for checking if a package is installed :)
#if(!require(somepackage)){
#    install.packages("somepackage")
#    library(somepackage)
#}

if (!require(tidycensus)) {
  install.packages("tidycensus")
}
if (!require(tidyverse)) {
  install.packages("tidyverse")
}
if (!require(dplyr)) {
  install.packages("dplyr")
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
}

#Load libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
library(ggplot2)

census_api_key("2547c95ce33b1ed0eec3aafd0fd8526a5bb9a22e")




### Obtain the Data

#Pull data set for specific variables
puma_data <- get_pums(variables = c("AGEP","SCHL","PINCP", "SEX")
                      ,state = "OR"
                      ,year = 2022)

#Rename the columns
puma_data <- puma_data %>% rename(Age = AGEP,
                                  EducationLevel = SCHL,
                                  Income = PINCP,
                                  Sex = SEX,
                                  PersonNumber = SPORDER)
head(puma_data[, 2:5])

#Remove columns that are not needed
puma_data <- puma_data %>% 
  select(-ST, -SERIALNO, -WGTP, -PWGTP)
head(puma_data)

#Convert education levels
puma_data <- puma_data %>%
  mutate(
    EducationLevel = case_when(
      EducationLevel == "bb" ~ "00",
      TRUE ~ EducationLevel
    ),
    EducationLevel = as.numeric(EducationLevel)
  )

#Make sure it is numeric
class(puma_data$EducationLevel)

##Group education level
puma_data <- puma_data %>%
  mutate(
     EducationGroup = cut(EducationLevel, breaks = c(0, 2, 11, 16, 20, 21, 22, 24, Inf),
                         labels = c("No schooling completed", "Less than High School",
                                    "High School", "high school diploma or GED",
                                    "Associates Degree", "Bachelors Degree",
                                    "Masters Degree", "Doctorate Degree"),
                         right = FALSE)
  )
unique(puma_data$EducationGroup)
puma_data[1:50,]

#Rearrange the columns
puma_data <- puma_data %>%
  select(EducationGroup, EducationLevel, Income, Age, Sex, PersonNumber)
head(puma_data)

#Drop age below 14
puma_data <- puma_data %>%
  filter(Age >= 14)
head(puma_data[puma_data$Age < 14,])

#Since we're refactoring the Education levels to groups, commenting this original out.
puma_data <- puma_data %>%
  mutate(
    EducationLevel = case_when(
      EducationLevel == 0 ~ "(less than 3 years old)",
      EducationLevel == 1 ~ "No schooling completed",
      EducationLevel == 2 ~ "Nursery School / Preschool",
      EducationLevel == 3 ~ "Kindergarten",
      EducationLevel == 4 ~ "Grade 1",
      EducationLevel == 5 ~ "Grade 2",
      EducationLevel == 6 ~ "Grade 3",
      EducationLevel == 7 ~ "Grade 4",
      EducationLevel == 8 ~ "Grade 5",
      EducationLevel == 9 ~ "Grade 6",
      EducationLevel == 10 ~ "Grade 7",
      EducationLevel == 11 ~ "Grade 8",
      EducationLevel == 12 ~ "Grade 9",
      EducationLevel == 13 ~ "Grade 10",
      EducationLevel == 14 ~ "Grade 11",
      EducationLevel == 15 ~ "Grade 12 - no diploma",
      EducationLevel == 16 ~ "Regular high school diploma",
      EducationLevel == 17 ~ "GED or alternative credential",
      EducationLevel == 18 ~ "Some college, but less than 1 year",
      EducationLevel == 19 ~ "1 or more years of college credit, no degree",
      EducationLevel == 20 ~ "Associates Degree",
      EducationLevel == 21 ~ "Bachelors Degree",
      EducationLevel == 22 ~ "Masters Degree",
      EducationLevel == 23 ~ "Professional Degree beyond a Bachelors Degree",
      EducationLevel == 24 ~ "Doctorate Degree",
      TRUE ~ as.character(EducationLevel)
    )
)
puma_data[1:100,]

#Identify gender
puma_data <- puma_data %>%
  mutate(
    Sex = case_when(
      Sex == 1 ~ "Male",
      Sex == 2 ~ "Female",
      TRUE ~ as.character(Sex)
    )
)
head(puma_data)





### Explore the data
#Summarizing the ranges of numerical 
summary(puma_data$Age) #Range of Age
summary(puma_data$Income) #Range of Age
summary(puma_data$EducationNumber) #Range of Education
table(puma_data$EducationLevel) #Count of each level of education

#Plot inceom distribution
ggplot(puma_data %>% filter(Income > 0), aes(Income)) +
  geom_histogram(binwidth = 10000) +
  scale_x_continuous(labels = function(x) paste0(x / 1000, "K")) +
  labs(title = "Income Distribution in Oregon"
       , x = "Income by $10k"
       , y = "Count of Individuals") +
  theme_minimal()
##[Reference on Variables for now](https://usa.ipums.org/usa/resources/codebooks/DataDict1822.pdf)




### Model the data

#Drop zero income from the data set
#Drop observation with 0 income (unemployed individuals)
puma_data <- puma_data[puma_data$Income != 0, ]
puma_data
head(puma_data)

# Does higher education mean higher income accounting for sex and age?
model_1 <- lm(Income ~ EducationGroup + Sex + Age, data = puma_data)
model_2 <- lm(Income ~ EducationGroup + Sex * Age, data = puma_data)
model_3 <- lm(Income ~ EducationGroup * Sex + Age, data = puma_data)
model_4 <- lm(Income ~ EducationGroup * Sex * Age, data = puma_data)

#Compare model 3 and 4
anova(model_3, model_4)

#The full model provides a significantly better fit.

#Compare model 2 and 4
anova(model_2, model_4)

#Model 4 is a better fit.

#Compare model 1 and 4
anova(model_1, model_4)


##Check the assumption

#Linearity and Homoscedasticity assumption
plot(fitted(model_4), residuals(model_4))

#Normality assumption
qqnorm(residuals(model_4))
qqline(residuals(model_4))

#Independence assumptions
plot(residuals(model_4))

#Fitted vs residuals plot exhibiting pattern

#Consider log transformation of the response variable:
#Logged models
model_5 <- lm(log(Income) ~ EducationGroup + Sex + Age, data = puma_data, na.action = na.exclude)
model_6 <- lm(log(Income) ~ EducationGroup + Sex * Age, data = puma_data, na.action = na.exclude)
model_7 <- lm(log(Income) ~ EducationGroup * Sex + Age, data = puma_data, na.action = na.exclude)
model_8 <- lm(log(Income) ~ EducationGroup * Sex * Age, data = puma_data, na.action = na.exclude)

#Compare models 7 and 8
anova(model_7, model_8)

#Compare models 6 and 8
anova(model_6, model_8)

#Compare models 5 and 8
anova(model_5, model_8)

#Model 8 is the best from 4 models


#Check the assumption
#Linear regression assumption
plot(fitted(model_8), residuals(model_8))

#Normality assumption
qqnorm(residuals(model_8))
qqline(residuals(model_8))

#Use predict() to fit the regression and find the average income for 50 YO males
new_data_1 <- data.frame(EducationGroup = c("No schooling completed","Less than High School", "High School", "high school diploma or GED","Associates Degree", "Bachelors Degree", "Masters Degree", "Doctorate Degree"),
                       Sex = "Male",
                       Age = 50)

mu_1 <- predict(model_8, newdata = new_data_1 , type = "response")
exp(mu_1)

#Use predict() to fit the regression and find the average income for 50 YO femails
new_data_2 <- data.frame(EducationGroup = c("No schooling completed","Less than High School", "High School", "high school diploma or GED","Associates Degree", "Bachelors Degree", "Masters Degree", "Doctorate Degree"),
                       Sex = "Female",
                       Age = 50)

mu_2 <- predict(model_8, newdata = new_data_2 , type = "response")
exp(mu_2)
```


